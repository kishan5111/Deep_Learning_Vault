{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Milestone Project 2: Skimlit\n\nThe purpose of this notebook is to build an NLP model to make reading medical abstracts easier.\n\nThe paper we're replicating(the source of the dataset that we'll be using) is available here: https://arxiv.org/abs/1710.06071\n\nAnd reading through the paper above, we see that the model architecture that they use to achieve their best reuslts is available at: https://arxiv.org/abs/1612.05251\n\n","metadata":{"id":"bxwqLldZQ0s7"}},{"cell_type":"markdown","source":"## Confirm access to GPU\n","metadata":{"id":"aRo6dkO5SZOq"}},{"cell_type":"code","source":"!nvidia -smi -L","metadata":{"id":"6G6n7GQYSrCm","outputId":"0ec05850-6eca-4b66-834a-603511f19eec","execution":{"iopub.status.busy":"2023-08-05T05:19:06.912881Z","iopub.execute_input":"2023-08-05T05:19:06.913289Z","iopub.status.idle":"2023-08-05T05:19:07.942733Z","shell.execute_reply.started":"2023-08-05T05:19:06.913250Z","shell.execute_reply":"2023-08-05T05:19:07.941383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get data\n\nSince we'll be replicating the paper above(Pubmed 200k RCT). let's download the dataset they used.\n\nWe can do so from the authors Github: https://github.com/Franck-Dernoncourt/pubmed-rct\n","metadata":{"id":"6U8nxP58SuLJ"}},{"cell_type":"code","source":"!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n!ls pubmed-rct","metadata":{"id":"po7dqZSWTlFz","outputId":"225ff17a-43a1-4ffa-995c-da79f333aa23","execution":{"iopub.status.busy":"2023-08-05T05:19:07.945367Z","iopub.execute_input":"2023-08-05T05:19:07.946090Z","iopub.status.idle":"2023-08-05T05:19:09.837260Z","shell.execute_reply.started":"2023-08-05T05:19:07.946045Z","shell.execute_reply":"2023-08-05T05:19:09.836062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check what files are in the Pubmed_20k dataset\n!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/","metadata":{"id":"HbykhC4DUMJ_","outputId":"e16e6624-402b-49ba-94b1-59bf117e98c5","execution":{"iopub.status.busy":"2023-08-05T05:19:09.839093Z","iopub.execute_input":"2023-08-05T05:19:09.839709Z","iopub.status.idle":"2023-08-05T05:19:10.779555Z","shell.execute_reply.started":"2023-08-05T05:19:09.839669Z","shell.execute_reply":"2023-08-05T05:19:10.778241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start our experiments using the 20k dataset with numbers replaced by \"@\" sign\ndata_dir = \"/kaggle/working/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"","metadata":{"id":"vNBwUv09VBFd","execution":{"iopub.status.busy":"2023-08-05T05:21:08.634467Z","iopub.execute_input":"2023-08-05T05:21:08.634835Z","iopub.status.idle":"2023-08-05T05:21:08.639246Z","shell.execute_reply.started":"2023-08-05T05:21:08.634804Z","shell.execute_reply":"2023-08-05T05:21:08.638237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check all of the filenames in the target directory\nimport os\nfilenames = [data_dir + filename for filename in os.listdir(data_dir)]\nfilenames","metadata":{"id":"Lj_fVWDPWOBU","outputId":"21978073-a7a7-4bf7-87fe-f5043e14bf90","execution":{"iopub.status.busy":"2023-08-05T05:21:09.180824Z","iopub.execute_input":"2023-08-05T05:21:09.181244Z","iopub.status.idle":"2023-08-05T05:21:09.189110Z","shell.execute_reply.started":"2023-08-05T05:21:09.181205Z","shell.execute_reply":"2023-08-05T05:21:09.188000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess the data\n\nNow we've got some text data, it's time to become one with it.\n\nAnd one the best ways to become one with the data is to..,\n> Visulize..Visualize..Visualize\n\nSo with that in the mind, let's write a function to read in all of the lines of a target text file","metadata":{"id":"BfSJ_qemWlf9"}},{"cell_type":"code","source":"# Create a function to read the lines of a document\ndef get_lines(filename):\n  \"\"\"\n  Reads filename (a text filename) and returns the lines of text as a list.\n\n  Args:\n    filename: a string containing the target filepath.\n\n  Returns:\n    A list of strings with one string per line from the target filename.\n  \"\"\"\n  with open(filename, \"r\") as f:\n    return f.readlines()","metadata":{"id":"UMK9ilpSXk_l","execution":{"iopub.status.busy":"2023-08-05T05:21:11.623086Z","iopub.execute_input":"2023-08-05T05:21:11.623815Z","iopub.status.idle":"2023-08-05T05:21:11.631090Z","shell.execute_reply.started":"2023-08-05T05:21:11.623781Z","shell.execute_reply":"2023-08-05T05:21:11.628334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's read in the training lines\ntrain_lines = get_lines(data_dir+\"train.txt\") #Read the lines with the training files\ntrain_lines[:20]","metadata":{"id":"g4QizVfSYaRE","outputId":"8494a4fb-576a-4883-b7c7-b19970dc47dd","execution":{"iopub.status.busy":"2023-08-05T05:21:12.610493Z","iopub.execute_input":"2023-08-05T05:21:12.610850Z","iopub.status.idle":"2023-08-05T05:21:12.681142Z","shell.execute_reply.started":"2023-08-05T05:21:12.610821Z","shell.execute_reply":"2023-08-05T05:21:12.680030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_lines)","metadata":{"id":"I7N68Z2CZCke","outputId":"3d2ebe17-8f9c-467f-f734-513ac94605f3","execution":{"iopub.status.busy":"2023-08-05T05:21:15.366960Z","iopub.execute_input":"2023-08-05T05:21:15.367350Z","iopub.status.idle":"2023-08-05T05:21:15.374517Z","shell.execute_reply.started":"2023-08-05T05:21:15.367318Z","shell.execute_reply":"2023-08-05T05:21:15.373525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's think about how we want our data to look...\n\nlet's try thiss...\n```\n[{'line_number': 0,\n  'target': 'BACKGROUND',\n  'text': \"Emotional eating is associated with overeating and the development of obesity .\\n\",\n  'total_lines': 11},\n  ...]\n```","metadata":{"id":"nNMGXKQweFUB"}},{"cell_type":"markdown","source":"Let's write a function which turns each of our datasets into the above format so we can continue to prepare our data for modelling.","metadata":{"id":"ArkKvM4Of_DU"}},{"cell_type":"code","source":"def preprocess_text_with_line_numbers(filename):\n  \"\"\"\n  Returns a list of dictionaries of abstract line data.\n\n  Takes in filename, reads it contents and sorts  through each line, extracting things\n  like the target label, the text of the sentence, how many sentences are in the current\n  abstract and what sentence number the target line is.\n  \"\"\"\n  input_lines = get_lines(filename) # get all lines from filename\n  abstract_lines = \"\"  # create an empty abstract\n  abstract_samples = [] # create an empty list of abstracts\n\n  # loop through each line in the target file\n  for line in input_lines:\n    if line.startswith(\"###\"): #check to see if the line is an ID line\n      abstract_id = line\n      abstract_lines = \"\" #reset the abstract string if the line is an id line\n    elif line.isspace(): # Check to see if line is a new line\n      abstract_line_split = abstract_lines.splitlines() #split abstract into separate lines\n\n      # Iterate through each line in a single abstract and count them at same time\n      for abstract_line_number , abstract_line in enumerate(abstract_line_split):\n        line_data = {} # creates a empty dictionary for each line\n        target_text_split = abstract_line.split(\"\\t\") # Split target labels from text\n        line_data[\"target\"] = target_text_split[0] # get the target label\n        line_data[\"text\"] = target_text_split[1].lower() #get the target text and lower it\n        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract\n        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are there in the target abstract? (start from 0)\n        abstract_samples.append(line_data) # add line data to abstract samples list\n\n    else: #if the above conditions aren't fulfilled, the line contains a labelled sentence\n      abstract_lines += line\n\n  return abstract_samples","metadata":{"id":"UrnW3cIrhMtH","execution":{"iopub.status.busy":"2023-08-05T05:21:20.626624Z","iopub.execute_input":"2023-08-05T05:21:20.626976Z","iopub.status.idle":"2023-08-05T05:21:20.636651Z","shell.execute_reply.started":"2023-08-05T05:21:20.626945Z","shell.execute_reply":"2023-08-05T05:21:20.635601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get data from file and preprocess it\n%time\ntrain_samples = preprocess_text_with_line_numbers(data_dir +\"train.txt\")\nval_samples = preprocess_text_with_line_numbers(data_dir +\"dev.txt\")\ntest_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\nprint(len(train_samples), len(test_samples), len(val_samples) )","metadata":{"id":"qCuwwaNQl-Jc","outputId":"f4c5a1a8-f8e2-4e6a-af86-570c49840aa2","execution":{"iopub.status.busy":"2023-08-05T05:21:31.994099Z","iopub.execute_input":"2023-08-05T05:21:31.994825Z","iopub.status.idle":"2023-08-05T05:21:32.778953Z","shell.execute_reply.started":"2023-08-05T05:21:31.994789Z","shell.execute_reply":"2023-08-05T05:21:32.777966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the first abstract of our training data\ntrain_samples[:10]","metadata":{"id":"eDdXLjDOoQuT","outputId":"9ccb0029-2739-4f30-b95c-9c1840911350","execution":{"iopub.status.busy":"2023-08-05T05:21:36.119416Z","iopub.execute_input":"2023-08-05T05:21:36.119781Z","iopub.status.idle":"2023-08-05T05:21:36.128123Z","shell.execute_reply.started":"2023-08-05T05:21:36.119750Z","shell.execute_reply":"2023-08-05T05:21:36.127189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that our data is the format of a list of dictionaries, how about we run it into\na DataFrame to further visualize it.","metadata":{"id":"gsTf0ZLEo4KZ"}},{"cell_type":"code","source":"import pandas as pd\ntrain_df  = pd.DataFrame(train_samples)\nval_df = pd.DataFrame(val_samples)\ntest_df = pd.DataFrame(test_samples)\ntrain_df.head(10)","metadata":{"id":"m2DHyJ6If6OH","outputId":"9727f2aa-4513-4ec5-d316-372c768e1dd6","execution":{"iopub.status.busy":"2023-08-05T05:21:36.130033Z","iopub.execute_input":"2023-08-05T05:21:36.130622Z","iopub.status.idle":"2023-08-05T05:21:36.594126Z","shell.execute_reply.started":"2023-08-05T05:21:36.130588Z","shell.execute_reply":"2023-08-05T05:21:36.592088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of labels in training data\ntrain_df.target.value_counts()","metadata":{"id":"X6TvqXy6gO7o","outputId":"652d4bb3-fb8d-4d73-ead3-1344f28e8e40","execution":{"iopub.status.busy":"2023-08-05T05:21:36.595895Z","iopub.execute_input":"2023-08-05T05:21:36.596336Z","iopub.status.idle":"2023-08-05T05:21:36.641288Z","shell.execute_reply.started":"2023-08-05T05:21:36.596299Z","shell.execute_reply":"2023-08-05T05:21:36.640183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the length of different lines\ntrain_df.total_lines.plot.hist();","metadata":{"id":"z2SwltvLgqEG","outputId":"30647980-b3a7-414a-fc64-83a236539898","execution":{"iopub.status.busy":"2023-08-05T05:21:36.642829Z","iopub.execute_input":"2023-08-05T05:21:36.643191Z","iopub.status.idle":"2023-08-05T05:21:36.992524Z","shell.execute_reply.started":"2023-08-05T05:21:36.643135Z","shell.execute_reply":"2023-08-05T05:21:36.991592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get lists of sentences","metadata":{"id":"47s7IUJgg3Ev"}},{"cell_type":"code","source":"# Convert abstract text lines into lists\ntrain_sentences = train_df[\"text\"].tolist()\nval_sentences = val_df[\"text\"].tolist()\ntest_sentences = test_df[\"text\"].tolist()\nlen(train_sentences), len(val_sentences), len(test_sentences)","metadata":{"id":"iqfeLzsDhAMC","outputId":"a51b085c-fd8a-4597-a73f-4ee0db28d808","execution":{"iopub.status.busy":"2023-08-05T05:21:36.995305Z","iopub.execute_input":"2023-08-05T05:21:36.995652Z","iopub.status.idle":"2023-08-05T05:21:37.010361Z","shell.execute_reply.started":"2023-08-05T05:21:36.995613Z","shell.execute_reply":"2023-08-05T05:21:37.009446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the 10 lines of training sentences\ntrain_sentences[:10]","metadata":{"id":"3gMqvZm2hf2_","outputId":"07be93f6-d412-459d-b759-c632c7776068","execution":{"iopub.status.busy":"2023-08-05T05:21:37.011574Z","iopub.execute_input":"2023-08-05T05:21:37.012558Z","iopub.status.idle":"2023-08-05T05:21:37.022000Z","shell.execute_reply.started":"2023-08-05T05:21:37.012518Z","shell.execute_reply":"2023-08-05T05:21:37.020850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make numeric labels (ML models require numeric labels)","metadata":{"id":"GZc5r184hpzY"}},{"cell_type":"code","source":"# One hot enode labels\nfrom sklearn.preprocessing import OneHotEncoder\none_hot_encoder = OneHotEncoder(sparse=False)\ntrain_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\nval_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1,1))\ntest_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n\n# Check what one hot encoded labels look like\ntrain_labels_one_hot","metadata":{"id":"syTsjcx9iYVs","outputId":"c2a87588-8325-445b-85b2-636794432034","execution":{"iopub.status.busy":"2023-08-05T05:21:37.023513Z","iopub.execute_input":"2023-08-05T05:21:37.023856Z","iopub.status.idle":"2023-08-05T05:21:37.139098Z","shell.execute_reply.started":"2023-08-05T05:21:37.023823Z","shell.execute_reply":"2023-08-05T05:21:37.138011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label encode labels","metadata":{"id":"sGH5BDdajOxJ"}},{"cell_type":"code","source":"# Extract labels (\"target\" columns) and encode them into integers\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\nval_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\ntest_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())","metadata":{"id":"MYPjJOFRkXUX","execution":{"iopub.status.busy":"2023-08-05T05:21:37.141274Z","iopub.execute_input":"2023-08-05T05:21:37.142027Z","iopub.status.idle":"2023-08-05T05:21:37.216619Z","shell.execute_reply.started":"2023-08-05T05:21:37.141988Z","shell.execute_reply":"2023-08-05T05:21:37.215688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels_encoded","metadata":{"id":"obitXfnzkrE-","outputId":"7bc075ee-d861-4d94-8d79-368e571ab4d9","execution":{"iopub.status.busy":"2023-08-05T05:21:37.218196Z","iopub.execute_input":"2023-08-05T05:21:37.218552Z","iopub.status.idle":"2023-08-05T05:21:37.226376Z","shell.execute_reply.started":"2023-08-05T05:21:37.218518Z","shell.execute_reply":"2023-08-05T05:21:37.225212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get class names and number of classes from LabelEncoder instance\nnum_classes = len(label_encoder.classes_)\nclass_names = label_encoder.classes_\nnum_classes, class_names","metadata":{"id":"ew8eKZf-lWu5","outputId":"2112a27e-9ae2-4ff4-d2fb-30d64f8bd4c0","execution":{"iopub.status.busy":"2023-08-05T05:21:37.227801Z","iopub.execute_input":"2023-08-05T05:21:37.228956Z","iopub.status.idle":"2023-08-05T05:21:37.245187Z","shell.execute_reply.started":"2023-08-05T05:21:37.228919Z","shell.execute_reply":"2023-08-05T05:21:37.244232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Starting a series of modelling experiments...\n\nAs usual, we're going to be trying out a bunch of different models and seeing which one works best\nAnd as always, we're going to start with a baseline(TF-IDF Multinomial Naive Bayes Classifier)","metadata":{"id":"F6zKW-u1mCI_"}},{"cell_type":"markdown","source":"## Model 0: Getting a baseline\n\nTo create our baseline, we'll use Sklearn's Multinomial Naive Bayes using the TF-IDF(Term frequency - inverse document frequency, i.e. tf-idf = tf * idf) formula to convert our words to numbers.\n\n> **Note:** It's common practice to use non-Dl algorithms as a baseline because of their speed and then later using Dl to see if you can improve upon them.","metadata":{"id":"vgzyNyNdpHSH"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import  TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline","metadata":{"id":"YoqC0Q-7pk1-","execution":{"iopub.status.busy":"2023-08-05T05:21:37.251609Z","iopub.execute_input":"2023-08-05T05:21:37.252257Z","iopub.status.idle":"2023-08-05T05:21:37.288836Z","shell.execute_reply.started":"2023-08-05T05:21:37.252223Z","shell.execute_reply":"2023-08-05T05:21:37.287965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a tokenization and modelling pipeline\nmodel_0 = Pipeline([\n                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n                    (\"clf\", MultinomialNB()) #MOdel the text\n])\n\n# Fit the pipeline to the training data\nmodel_0.fit(train_sentences, train_labels_encoded)","metadata":{"id":"LELwz7KHp5LM","outputId":"7a259e87-09a4-4548-e019-6ca1379201de","execution":{"iopub.status.busy":"2023-08-05T05:21:37.290119Z","iopub.execute_input":"2023-08-05T05:21:37.290491Z","iopub.status.idle":"2023-08-05T05:21:44.970119Z","shell.execute_reply.started":"2023-08-05T05:21:37.290458Z","shell.execute_reply":"2023-08-05T05:21:44.969156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\nmodel_0.score(val_sentences, val_labels_encoded)","metadata":{"id":"7jnYyHsqqkrL","outputId":"4fe16b37-4070-4ca5-8ee4-cc4ce3063ebe","execution":{"iopub.status.busy":"2023-08-05T05:21:44.974372Z","iopub.execute_input":"2023-08-05T05:21:44.977071Z","iopub.status.idle":"2023-08-05T05:21:45.910776Z","shell.execute_reply.started":"2023-08-05T05:21:44.977034Z","shell.execute_reply":"2023-08-05T05:21:45.909671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions using our baseline model\nbaseline_preds = model_0.predict(val_sentences)\nbaseline_preds","metadata":{"id":"MsgnqqHoqwiZ","outputId":"89ac8f9f-2d2c-452b-be3a-7f243b82a3e3","execution":{"iopub.status.busy":"2023-08-05T05:21:45.912392Z","iopub.execute_input":"2023-08-05T05:21:45.912821Z","iopub.status.idle":"2023-08-05T05:21:46.722839Z","shell.execute_reply.started":"2023-08-05T05:21:45.912786Z","shell.execute_reply":"2023-08-05T05:21:46.721754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download helper functions script","metadata":{"id":"8q8N5V1tsDoO"}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py","metadata":{"id":"5r78BRQ0sSLR","outputId":"f3466952-345d-40be-bfa5-92cb7e7e9be3","execution":{"iopub.status.busy":"2023-08-05T05:21:46.724465Z","iopub.execute_input":"2023-08-05T05:21:46.724829Z","iopub.status.idle":"2023-08-05T05:21:47.982135Z","shell.execute_reply.started":"2023-08-05T05:21:46.724793Z","shell.execute_reply":"2023-08-05T05:21:47.980935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from helper_functions import calculate_results","metadata":{"id":"FYP7e7fXsiMm","execution":{"iopub.status.busy":"2023-08-05T05:21:47.984016Z","iopub.execute_input":"2023-08-05T05:21:47.984750Z","iopub.status.idle":"2023-08-05T05:21:58.335004Z","shell.execute_reply.started":"2023-08-05T05:21:47.984709Z","shell.execute_reply":"2023-08-05T05:21:58.334024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calcluate baseline results\nbaseline_results = calculate_results(y_true=val_labels_encoded,\n                                     y_pred = baseline_preds)","metadata":{"id":"py5ixQFvsvEH","execution":{"iopub.status.busy":"2023-08-05T05:21:58.337295Z","iopub.execute_input":"2023-08-05T05:21:58.338512Z","iopub.status.idle":"2023-08-05T05:21:58.357294Z","shell.execute_reply.started":"2023-08-05T05:21:58.338470Z","shell.execute_reply":"2023-08-05T05:21:58.356364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_results","metadata":{"id":"NzkSyy7wtEHG","outputId":"73842fc4-cf42-45e3-d3cc-b7f0661209cf","execution":{"iopub.status.busy":"2023-08-05T05:21:58.358935Z","iopub.execute_input":"2023-08-05T05:21:58.359340Z","iopub.status.idle":"2023-08-05T05:21:58.368263Z","shell.execute_reply.started":"2023-08-05T05:21:58.359303Z","shell.execute_reply":"2023-08-05T05:21:58.365201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing our data(the text) for deep sequence model\n\nBefore we start building deeper models, we've got to create vectorization and embedding layers.  \n","metadata":{"id":"F54S4MdXtF5-"}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers","metadata":{"id":"DXKtMgOJWv5P","execution":{"iopub.status.busy":"2023-08-05T05:21:58.369721Z","iopub.execute_input":"2023-08-05T05:21:58.370759Z","iopub.status.idle":"2023-08-05T05:21:58.376497Z","shell.execute_reply.started":"2023-08-05T05:21:58.370724Z","shell.execute_reply":"2023-08-05T05:21:58.375347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How long is each sentence on average?\nsent_lens = [len(sentence.split()) for sentence in train_sentences]\navg_sent_len = np.mean(sent_lens)\navg_sent_len","metadata":{"id":"Q2ehkWkjZfhP","outputId":"3a42be13-eb81-471a-f884-4425ba66bb70","execution":{"iopub.status.busy":"2023-08-05T05:21:58.377899Z","iopub.execute_input":"2023-08-05T05:21:58.378609Z","iopub.status.idle":"2023-08-05T05:21:58.757222Z","shell.execute_reply.started":"2023-08-05T05:21:58.378572Z","shell.execute_reply":"2023-08-05T05:21:58.756087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What's the distribution look like?\nimport matplotlib.pyplot as plt\nplt.hist(sent_lens, bins=20);","metadata":{"id":"fCXEpWspZ1Jy","outputId":"630954cb-ace3-42a2-d41b-8dcce335729e","execution":{"iopub.status.busy":"2023-08-05T05:21:58.758823Z","iopub.execute_input":"2023-08-05T05:21:58.759208Z","iopub.status.idle":"2023-08-05T05:21:59.940629Z","shell.execute_reply.started":"2023-08-05T05:21:58.759150Z","shell.execute_reply":"2023-08-05T05:21:59.939422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How long of a sentence length covers 95% of examples?\noutput_seq_len = int(np.percentile(sent_lens, 95))\noutput_seq_len","metadata":{"id":"7CLX507Gaffe","outputId":"06c6ad7e-5628-4984-b718-4627911d1b2c","execution":{"iopub.status.busy":"2023-08-05T05:21:59.945050Z","iopub.execute_input":"2023-08-05T05:21:59.947958Z","iopub.status.idle":"2023-08-05T05:22:00.000734Z","shell.execute_reply.started":"2023-08-05T05:21:59.947920Z","shell.execute_reply":"2023-08-05T05:21:59.999660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Maximum sequence length in the training set?\nmax(sent_lens)","metadata":{"id":"FPxcF2l0beXS","outputId":"b964fbe7-34d3-4d65-cb24-a078700dd166","execution":{"iopub.status.busy":"2023-08-05T05:22:00.005043Z","iopub.execute_input":"2023-08-05T05:22:00.007735Z","iopub.status.idle":"2023-08-05T05:22:00.023938Z","shell.execute_reply.started":"2023-08-05T05:22:00.007697Z","shell.execute_reply":"2023-08-05T05:22:00.022702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Vectorizer","metadata":{"id":"XiMzLv6qb6zC"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n# Use the default TextVectorization parameters\ntext_vectorizer = TextVectorization(max_tokens=68000, #how many words in vocabulary(automatically add<OOV>)\n                                    output_sequence_length=55)","metadata":{"id":"BT3Tfa8LcOjU","execution":{"iopub.status.busy":"2023-08-05T05:22:00.028982Z","iopub.execute_input":"2023-08-05T05:22:00.031530Z","iopub.status.idle":"2023-08-05T05:22:05.042174Z","shell.execute_reply.started":"2023-08-05T05:22:00.031494Z","shell.execute_reply":"2023-08-05T05:22:05.041220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adapt text vectorizer to training sentences\ntext_vectorizer.adapt(train_sentences)","metadata":{"id":"iDhlgxVBdePA","execution":{"iopub.status.busy":"2023-08-05T05:22:05.043665Z","iopub.execute_input":"2023-08-05T05:22:05.044011Z","iopub.status.idle":"2023-08-05T05:22:27.856571Z","shell.execute_reply.started":"2023-08-05T05:22:05.043976Z","shell.execute_reply":"2023-08-05T05:22:27.855508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test out text vectorizer on random sentences\nimport random\ntarget_sentence = random.choice(train_sentences)\nprint(f\"Text:\\n {target_sentence}\")\nprint(f\"\\nLength of text: {len(target_sentence.split())}\")\nprint(f\"\\nVectorized text: {text_vectorizer([target_sentence])}\")","metadata":{"id":"MInR4Su3d0V6","outputId":"abab0557-4bf0-4a20-c973-68acf3e7bf81","execution":{"iopub.status.busy":"2023-08-05T05:22:27.858752Z","iopub.execute_input":"2023-08-05T05:22:27.859237Z","iopub.status.idle":"2023-08-05T05:22:27.961412Z","shell.execute_reply.started":"2023-08-05T05:22:27.859200Z","shell.execute_reply":"2023-08-05T05:22:27.960232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many words in our training vocabulary\nrct_20k_text_vocab = text_vectorizer.get_vocabulary()\nprint(f\"Number of words in vocab: {len(rct_20k_text_vocab)}\")\nprint(f\"Most common words in the vocab: {rct_20k_text_vocab[:5]}\")\nprint(f\"Least common words in the vocab: {rct_20k_text_vocab[-5:]}\")","metadata":{"id":"9oaiJzypeg8K","outputId":"ce47706d-702b-4d04-b2d6-c6bad005d4ba","execution":{"iopub.status.busy":"2023-08-05T05:22:27.962763Z","iopub.execute_input":"2023-08-05T05:22:27.963132Z","iopub.status.idle":"2023-08-05T05:22:28.179515Z","shell.execute_reply.started":"2023-08-05T05:22:27.963085Z","shell.execute_reply":"2023-08-05T05:22:28.178339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the config of our text vectorizer\ntext_vectorizer.get_config()","metadata":{"id":"AXccGGSCfrJ-","outputId":"f74a474d-dddf-4785-b6d0-054c7610419d","execution":{"iopub.status.busy":"2023-08-05T05:22:28.180977Z","iopub.execute_input":"2023-08-05T05:22:28.181350Z","iopub.status.idle":"2023-08-05T05:22:28.188443Z","shell.execute_reply.started":"2023-08-05T05:22:28.181305Z","shell.execute_reply":"2023-08-05T05:22:28.187521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a custom text embedding","metadata":{"id":"yV2LBRQag4k2"}},{"cell_type":"code","source":"from tensorflow.keras  import layers\n\nembedding = layers.Embedding(input_dim = len(rct_20k_text_vocab), #set input length\n                             output_dim = 128, #output shape\n                             mask_zero=True, # use masking to handle variable sequence lengths(save space)\n                             input_length= 55, #how long is each input\n                             name=\"token_embedding\")\nembedding","metadata":{"id":"WK4MZ2Pys2BK","outputId":"cfda3ccb-3213-4f40-fde0-8270fd976c4f","execution":{"iopub.status.busy":"2023-08-05T05:22:28.201037Z","iopub.execute_input":"2023-08-05T05:22:28.201598Z","iopub.status.idle":"2023-08-05T05:22:28.211644Z","shell.execute_reply.started":"2023-08-05T05:22:28.201561Z","shell.execute_reply":"2023-08-05T05:22:28.210484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show example embedding\nprint(f\"Sentence before vectorization: \\n {target_sentence}\\n\")\nvectorized_sentence = text_vectorizer([target_sentence])\nprint(f\"Sentence after vectorization (before embedding): \\n {vectorized_sentence}\\n\")\nembedded_sentence = embedding(vectorized_sentence)\nprint(f\"Sentence after embedding: \\n {embedded_sentence}\\n\")\nprint(f\"Embedded sentence shape: {embedded_sentence.shape}\")","metadata":{"id":"U54ulxt9gyXC","outputId":"c8b9c703-bc18-48f9-d092-02ed64f424a8","execution":{"iopub.status.busy":"2023-08-05T05:22:28.213267Z","iopub.execute_input":"2023-08-05T05:22:28.213678Z","iopub.status.idle":"2023-08-05T05:22:28.271769Z","shell.execute_reply.started":"2023-08-05T05:22:28.213644Z","shell.execute_reply":"2023-08-05T05:22:28.270646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating datasets (making sure our data loads as fast as possible)\n\n","metadata":{"id":"vfalNMQyj-1a"}},{"cell_type":"code","source":"# Turn our data into TensorFlow Datasets\ntrain_dataset =  tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n\ntrain_dataset","metadata":{"id":"VJExrMi-lGlj","outputId":"09c17af0-20dd-4075-d33a-de5340bfb482","execution":{"iopub.status.busy":"2023-08-05T05:22:28.273986Z","iopub.execute_input":"2023-08-05T05:22:28.275088Z","iopub.status.idle":"2023-08-05T05:22:29.257240Z","shell.execute_reply.started":"2023-08-05T05:22:28.275052Z","shell.execute_reply":"2023-08-05T05:22:29.256204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take the TensorSliceDataset's and turn them into prefetch dataset\ntrain_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\nvalid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"id":"koNe9Uu7mm7q","execution":{"iopub.status.busy":"2023-08-05T05:22:29.258690Z","iopub.execute_input":"2023-08-05T05:22:29.259037Z","iopub.status.idle":"2023-08-05T05:22:29.272464Z","shell.execute_reply.started":"2023-08-05T05:22:29.259008Z","shell.execute_reply":"2023-08-05T05:22:29.271532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"id":"prsjkHlLnuC7","outputId":"c1203dfb-1a23-4da3-ac11-bb016a40d1c7","execution":{"iopub.status.busy":"2023-08-05T05:22:29.275224Z","iopub.execute_input":"2023-08-05T05:22:29.276121Z","iopub.status.idle":"2023-08-05T05:22:29.283623Z","shell.execute_reply.started":"2023-08-05T05:22:29.276087Z","shell.execute_reply":"2023-08-05T05:22:29.282644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 1: Conv1D with token embeddings","metadata":{"id":"zgDJ3xlfn1O-"}},{"cell_type":"code","source":"from tensorflow.keras import layers\n\n# Build the model\ninputs = layers.Input(shape=(1, ), dtype=tf.string)\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = layers.Conv1D(filters=64, kernel_size=5,  activation=\"relu\", padding =\"same\")(x)\nx = layers.GlobalMaxPool1D()(x)\noutputs = layers.Dense(5, activation=\"softmax\")(x)\n\nmodel_1 = tf.keras.Model(inputs, outputs, name=\"model_1_Conv1D\")\n\n# Compile the model\nmodel_1.compile(loss=tf.keras.losses.categorical_crossentropy,\n                metrics=[\"accuracy\"],\n                optimizer=tf.keras.optimizers.Adam())","metadata":{"id":"K1Kk3piaoQH8","execution":{"iopub.status.busy":"2023-08-05T05:22:29.285464Z","iopub.execute_input":"2023-08-05T05:22:29.285724Z","iopub.status.idle":"2023-08-05T05:22:29.384138Z","shell.execute_reply.started":"2023-08-05T05:22:29.285701Z","shell.execute_reply":"2023-08-05T05:22:29.383106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.summary()","metadata":{"id":"cfpTIe5Mp0cQ","outputId":"c1e9624b-b3ac-446a-a93e-4f50546ef422","execution":{"iopub.status.busy":"2023-08-05T05:22:29.386236Z","iopub.execute_input":"2023-08-05T05:22:29.386819Z","iopub.status.idle":"2023-08-05T05:22:29.413897Z","shell.execute_reply.started":"2023-08-05T05:22:29.386773Z","shell.execute_reply":"2023-08-05T05:22:29.413099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nhistory_model_1 = model_1.fit(train_dataset,\n                              validation_data=(valid_dataset),\n                              epochs=3,\n                              steps_per_epoch=int(0.1 * len(train_dataset)),\n                              validation_steps=int(len(valid_dataset) * 0.10))","metadata":{"id":"whrL8CMxp4rS","outputId":"7b4ab6e0-7c7c-4080-b53a-d51bc97354b5","execution":{"iopub.status.busy":"2023-08-05T05:22:29.414879Z","iopub.execute_input":"2023-08-05T05:22:29.415413Z","iopub.status.idle":"2023-08-05T05:23:52.118915Z","shell.execute_reply.started":"2023-08-05T05:22:29.415385Z","shell.execute_reply":"2023-08-05T05:23:52.117759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on validation data\nmodel_1.evaluate(valid_dataset)","metadata":{"id":"_6rig1r7qUrC","outputId":"f08e02a5-8730-4aeb-ba9f-17a5e7f65e8f","execution":{"iopub.status.busy":"2023-08-05T05:23:52.120809Z","iopub.execute_input":"2023-08-05T05:23:52.121207Z","iopub.status.idle":"2023-08-05T05:23:57.243212Z","shell.execute_reply.started":"2023-08-05T05:23:52.121151Z","shell.execute_reply":"2023-08-05T05:23:57.242211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make prediction(model_predicts prediction probabilities for each class)\nmodel_1_pred_probs =  model_1.predict(valid_dataset)\nmodel_1_preds = tf.argmax(model_1_pred_probs, axis=1)\nmodel_1_preds","metadata":{"id":"C2RKM635tJav","outputId":"084d4226-5a56-48f0-e5cb-8e7b98b30798","execution":{"iopub.status.busy":"2023-08-05T05:23:57.244890Z","iopub.execute_input":"2023-08-05T05:23:57.245278Z","iopub.status.idle":"2023-08-05T05:24:00.157362Z","shell.execute_reply.started":"2023-08-05T05:23:57.245235Z","shell.execute_reply":"2023-08-05T05:24:00.156162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_preds[:10]","metadata":{"id":"LOT23cHzt3fY","outputId":"77d3ced5-f584-4ba6-a65b-b2344fa215f4","execution":{"iopub.status.busy":"2023-08-05T05:24:00.158741Z","iopub.execute_input":"2023-08-05T05:24:00.159153Z","iopub.status.idle":"2023-08-05T05:24:00.170478Z","shell.execute_reply.started":"2023-08-05T05:24:00.159113Z","shell.execute_reply":"2023-08-05T05:24:00.169246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate model_1 results\nmodel_1_results= calculate_results(y_pred=model_1_preds,\n                                   y_true=val_labels_encoded)","metadata":{"id":"1PAvSKkiuBOq","execution":{"iopub.status.busy":"2023-08-05T05:24:00.172573Z","iopub.execute_input":"2023-08-05T05:24:00.172875Z","iopub.status.idle":"2023-08-05T05:24:00.194538Z","shell.execute_reply.started":"2023-08-05T05:24:00.172839Z","shell.execute_reply":"2023-08-05T05:24:00.193602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_results, baseline_results\n","metadata":{"id":"R1YN-R1GuTxi","outputId":"a3be3b63-1c16-43ef-ce5f-eba51b3999db","execution":{"iopub.status.busy":"2023-08-05T05:24:00.195830Z","iopub.execute_input":"2023-08-05T05:24:00.196286Z","iopub.status.idle":"2023-08-05T05:24:00.204335Z","shell.execute_reply.started":"2023-08-05T05:24:00.196249Z","shell.execute_reply":"2023-08-05T05:24:00.203225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 2: Feature extraction with pretrained  token embedding\nNow let's use pretrained word embeddings from TensorFlow Hub, more specifically the universal sentence encoder: https://tfhub.dev/google/universal-sentence-encoder/4","metadata":{"id":"eKjNnYbnurgS"}},{"cell_type":"code","source":"import tensorflow_hub as hub\n# Create a Keras layer using the USE pretrained layer from tensorflowhub\nsentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\",\n                                        input_shape=[],\n                                        dtype=tf.string,\n                                        trainable= False,\n                                        name=\"USE-large\")","metadata":{"id":"wcbiupCYvrlq","execution":{"iopub.status.busy":"2023-08-05T05:24:00.206012Z","iopub.execute_input":"2023-08-05T05:24:00.206758Z","iopub.status.idle":"2023-08-05T05:24:21.196144Z","shell.execute_reply.started":"2023-08-05T05:24:00.206721Z","shell.execute_reply":"2023-08-05T05:24:21.195115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\nmodel_2 = tf.keras.Sequential([\n    sentence_encoder_layer,\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dense(5, activation=\"softmax\")\n])\n\nmodel_2.compile(loss=\"categorical_crossentropy\",\n                optimizer=\"adam\",\n                metrics=[\"accuracy\"])\n\nmodel_2.summary()","metadata":{"id":"QaXFdO2PxpV8","outputId":"89eb4d0b-c807-4af5-bdb1-6d1ab4617b86","execution":{"iopub.status.busy":"2023-08-05T05:24:21.197481Z","iopub.execute_input":"2023-08-05T05:24:21.197842Z","iopub.status.idle":"2023-08-05T05:24:24.188512Z","shell.execute_reply.started":"2023-08-05T05:24:21.197809Z","shell.execute_reply":"2023-08-05T05:24:24.187513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_model_2 = model_2.fit(train_dataset,\n                              epochs=3,\n                              validation_data=(valid_dataset),\n                              steps_per_epoch=int(0.1 * len(train_dataset)),\n                              validation_steps=int(0.1 * len(valid_dataset)))","metadata":{"id":"051sf8G5yk3L","outputId":"2bd18c37-b1e1-44b2-ca1f-9a6dad364de3","execution":{"iopub.status.busy":"2023-08-05T05:24:24.190132Z","iopub.execute_input":"2023-08-05T05:24:24.190995Z","iopub.status.idle":"2023-08-05T05:27:15.139889Z","shell.execute_reply.started":"2023-08-05T05:24:24.190958Z","shell.execute_reply":"2023-08-05T05:27:15.138780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.evaluate(valid_dataset)","metadata":{"id":"dW_MGxoUzZ11","outputId":"d2d79ac9-48cc-4bd1-94b5-8c795be8ce87","execution":{"iopub.status.busy":"2023-08-05T05:27:15.141589Z","iopub.execute_input":"2023-08-05T05:27:15.142063Z","iopub.status.idle":"2023-08-05T05:27:59.631248Z","shell.execute_reply.started":"2023-08-05T05:27:15.142024Z","shell.execute_reply":"2023-08-05T05:27:59.630090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions with feature extraction model\nmodel_2_pred_probs = model_2.predict(valid_dataset)\nmodel_2_preds = tf.argmax(model_2_pred_probs, axis=1)\nmodel_2_preds[:10]","metadata":{"id":"Q5DRSV5U1SbA","outputId":"04b02d8f-a495-475e-bbb6-2dfdf10dd25c","execution":{"iopub.status.busy":"2023-08-05T05:27:59.633038Z","iopub.execute_input":"2023-08-05T05:27:59.633461Z","iopub.status.idle":"2023-08-05T05:28:53.288237Z","shell.execute_reply.started":"2023-08-05T05:27:59.633422Z","shell.execute_reply":"2023-08-05T05:28:53.287117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calulate results Tfhub pretrained embeddings results on val set\nmodel_2_results = calculate_results(y_true=val_labels_encoded,\n                                    y_pred=model_2_preds)\nmodel_2_results","metadata":{"id":"ezVcHm-d1256","outputId":"d4764d0a-3d63-428b-b1ed-03d788f68451","execution":{"iopub.status.busy":"2023-08-05T05:28:53.289759Z","iopub.execute_input":"2023-08-05T05:28:53.290238Z","iopub.status.idle":"2023-08-05T05:28:53.314495Z","shell.execute_reply.started":"2023-08-05T05:28:53.290203Z","shell.execute_reply":"2023-08-05T05:28:53.313380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 3: Conv1D with character embeddings\nThe paper which we're replicating states they used a combination of token and character-level embeddings.\n\nPreviously we've token-level embeddings but we'll need to do similar steps for characters if we want to use char-level embeddings.","metadata":{"id":"vaLKzFUR2OsD"}},{"cell_type":"markdown","source":"### Creating a character-level tokenizer","metadata":{"id":"cDGRSVWV3Vos"}},{"cell_type":"code","source":"train_sentences[:5]","metadata":{"id":"1kccHy5V38bs","outputId":"36040721-89fe-41b6-b741-6fe4de86cfb7","execution":{"iopub.status.busy":"2023-08-05T05:28:53.315969Z","iopub.execute_input":"2023-08-05T05:28:53.317425Z","iopub.status.idle":"2023-08-05T05:28:53.324519Z","shell.execute_reply.started":"2023-08-05T05:28:53.317388Z","shell.execute_reply":"2023-08-05T05:28:53.323394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make function to split sentences into characters\ndef split_chars(text):\n  return \" \".join(list(text))","metadata":{"id":"ymeNfv9p3_SS","execution":{"iopub.status.busy":"2023-08-05T05:28:53.326113Z","iopub.execute_input":"2023-08-05T05:28:53.326478Z","iopub.status.idle":"2023-08-05T05:28:53.333955Z","shell.execute_reply.started":"2023-08-05T05:28:53.326444Z","shell.execute_reply":"2023-08-05T05:28:53.333005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text splitting non-character-level sequence into characters\nsplit_chars(train_sentences[0])","metadata":{"id":"Kwsamyuo4ZDk","outputId":"fa22b1dc-8810-4798-c1f5-20233e021f66","execution":{"iopub.status.busy":"2023-08-05T05:28:53.335445Z","iopub.execute_input":"2023-08-05T05:28:53.335784Z","iopub.status.idle":"2023-08-05T05:28:53.345440Z","shell.execute_reply.started":"2023-08-05T05:28:53.335750Z","shell.execute_reply":"2023-08-05T05:28:53.344292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split sequence level data splits into character level data splits\ntrain_chars = [split_chars(sentence) for sentence in train_sentences]\nvalid_chars = [split_chars(sentence) for sentence in val_sentences]\ntest_chars = [split_chars(sentence)for sentence in test_sentences]\ntrain_chars[:5]","metadata":{"id":"23igsHUq4sys","outputId":"6079fd0d-9bc4-4ae4-c4c7-9520e4ed7a7e","execution":{"iopub.status.busy":"2023-08-05T05:28:53.346935Z","iopub.execute_input":"2023-08-05T05:28:53.347447Z","iopub.status.idle":"2023-08-05T05:28:55.070307Z","shell.execute_reply.started":"2023-08-05T05:28:53.347414Z","shell.execute_reply":"2023-08-05T05:28:55.069141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What's the average character length?\nchars_length = [len(sentence) for sentence in train_sentences]\nmean_char_len = np.mean(chars_length)\nmean_char_len","metadata":{"id":"LwSEt_XC5b9O","outputId":"b9967cce-124a-42f2-fbba-19c174b3f75d","execution":{"iopub.status.busy":"2023-08-05T05:28:55.071619Z","iopub.execute_input":"2023-08-05T05:28:55.072059Z","iopub.status.idle":"2023-08-05T05:28:55.136531Z","shell.execute_reply.started":"2023-08-05T05:28:55.072024Z","shell.execute_reply":"2023-08-05T05:28:55.135473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the distribution of our sentence ar a character level\nimport matplotlib.pyplot as plt\nplt.hist(chars_length, bins=7)","metadata":{"id":"n5c60iwn5xYb","outputId":"5af4100f-f499-470f-92d2-2677adba2c63","execution":{"iopub.status.busy":"2023-08-05T05:28:55.138235Z","iopub.execute_input":"2023-08-05T05:28:55.138584Z","iopub.status.idle":"2023-08-05T05:28:56.269210Z","shell.execute_reply.started":"2023-08-05T05:28:55.138551Z","shell.execute_reply":"2023-08-05T05:28:56.268115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find what character length cover 95% of sequences\noutput_seq_char_len = int(np.percentile(chars_length, 95))\noutput_seq_char_len","metadata":{"id":"Az8iV4U76BgV","outputId":"85846851-6c86-4cad-ca12-c95a15734e46","execution":{"iopub.status.busy":"2023-08-05T05:28:56.270770Z","iopub.execute_input":"2023-08-05T05:28:56.271117Z","iopub.status.idle":"2023-08-05T05:28:56.303051Z","shell.execute_reply.started":"2023-08-05T05:28:56.271084Z","shell.execute_reply":"2023-08-05T05:28:56.301979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all keyboard characters\nimport string\nalphabet = string.ascii_lowercase + string.digits + string.punctuation\nprint(len(alphabet), alphabet)","metadata":{"id":"puvcSSfJ6Yh0","outputId":"f8be8886-a6fb-4e8b-d1e6-8ea2331d9385","execution":{"iopub.status.busy":"2023-08-05T05:28:56.304489Z","iopub.execute_input":"2023-08-05T05:28:56.304916Z","iopub.status.idle":"2023-08-05T05:28:56.323392Z","shell.execute_reply.started":"2023-08-05T05:28:56.304882Z","shell.execute_reply":"2023-08-05T05:28:56.322104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create char-level tokenvectorier instance\nNUM_CHAR_TOKENS = len(alphabet) + 2 #add 3 for space and OOV token(OOV = out of vocab)\nchar_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n                                    output_sequence_length = output_seq_char_len,\n                                    # standardize=None,\n                                    name=\"char_vectorizer\")","metadata":{"id":"zYMkWNaB7euU","execution":{"iopub.status.busy":"2023-08-05T05:28:56.324951Z","iopub.execute_input":"2023-08-05T05:28:56.325337Z","iopub.status.idle":"2023-08-05T05:28:56.339139Z","shell.execute_reply.started":"2023-08-05T05:28:56.325303Z","shell.execute_reply":"2023-08-05T05:28:56.338209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adapt to training chars\nchar_vectorizer.adapt(train_chars)","metadata":{"id":"KNf9F-IO8lU_","execution":{"iopub.status.busy":"2023-08-05T05:28:56.340697Z","iopub.execute_input":"2023-08-05T05:28:56.341093Z","iopub.status.idle":"2023-08-05T05:29:19.982800Z","shell.execute_reply.started":"2023-08-05T05:28:56.341034Z","shell.execute_reply":"2023-08-05T05:29:19.981771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check character vocab stats\nchar_vocab = char_vectorizer.get_vocabulary()\nprint(f\"Number of different characters in character vocab: {len(char_vocab)}\")\nprint(f\"5 Most common characters: {char_vocab[:5]}\")\nprint(f\"5 least common characters: {char_vocab[-5:]}\")","metadata":{"id":"2-aQV3A98tPD","outputId":"69649f46-fffc-463d-a976-a0a10ca0874c","execution":{"iopub.status.busy":"2023-08-05T05:29:19.984868Z","iopub.execute_input":"2023-08-05T05:29:19.985278Z","iopub.status.idle":"2023-08-05T05:29:19.992838Z","shell.execute_reply.started":"2023-08-05T05:29:19.985232Z","shell.execute_reply":"2023-08-05T05:29:19.991903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test out character vectorizer\nrandom_train_chars = random.choice(train_chars)\nprint(f\"Charified text:\\n {random_train_chars}\")\nprint(f\"\\nLength of random_train_chars: {len(random_train_chars.split())}\")\nvectorized_chars = char_vectorizer([random_train_chars])\nprint(f\"\\nVectorized chars:\\n {vectorized_chars}\")\nprint(f\"\\nLength of Vectorized chars: {len(vectorized_chars[0])}\")","metadata":{"id":"ZEaitie49gvV","outputId":"0033268d-be77-485a-de20-8fb98b4c2d65","execution":{"iopub.status.busy":"2023-08-05T05:29:19.994057Z","iopub.execute_input":"2023-08-05T05:29:19.994976Z","iopub.status.idle":"2023-08-05T05:29:20.018631Z","shell.execute_reply.started":"2023-08-05T05:29:19.994940Z","shell.execute_reply":"2023-08-05T05:29:20.017666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Character level embedding","metadata":{"id":"brZtkVFNkXgm"}},{"cell_type":"code","source":"from tensorflow.keras import layers\n\nchar_embedding = layers.Embedding(input_dim = len(char_vocab),\n                                  output_dim = 25, #this is size of the char embed in paper\n                                  mask_zero=True,\n                                  name = \"character_embedding\")","metadata":{"id":"YRfa-KCblFKh","execution":{"iopub.status.busy":"2023-08-05T05:29:20.020469Z","iopub.execute_input":"2023-08-05T05:29:20.020755Z","iopub.status.idle":"2023-08-05T05:29:20.026645Z","shell.execute_reply.started":"2023-08-05T05:29:20.020731Z","shell.execute_reply":"2023-08-05T05:29:20.025643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show sample char embedding\nprint(f\"Sentence before vectorization: \\n {random_train_chars}\\n\")\nvectorized_chars = char_vectorizer([random_train_chars])\nprint(f\"Sentence after vectorization(before embedding):\\n {vectorized_chars}\\n\")\nchar_embedded = char_embedding(vectorized_chars)\nprint(f\"Sentence after embedding:\\n{char_embedded}\\n\")\nprint(f\"Embedded char sentence shape:{char_embedded.shape}\")","metadata":{"id":"YFCfkpfWkn6H","outputId":"30db1860-f7dc-4fd8-f460-e9f88eee01a6","execution":{"iopub.status.busy":"2023-08-05T05:29:20.028027Z","iopub.execute_input":"2023-08-05T05:29:20.028605Z","iopub.status.idle":"2023-08-05T05:29:20.058304Z","shell.execute_reply.started":"2023-08-05T05:29:20.028571Z","shell.execute_reply":"2023-08-05T05:29:20.057236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building a Conv1D model to fit on character embeddings","metadata":{"id":"Yqu6mnusnGWk"}},{"cell_type":"code","source":"# Make Conv1D on char embedding\nfrom tensorflow.keras import layers\n\ninputs = layers.Input(shape=(1,), dtype=tf.string)\nchar_vectors = char_vectorizer(inputs)\nchar_embeddings = char_embedding(char_vectors)\nx = layers.Conv1D(filters=64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\nx = layers.GlobalMaxPool1D()(x)\noutputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n\nmodel_3 = tf.keras.Model(inputs=inputs, outputs=outputs)\nmodel_3.summary()","metadata":{"id":"hfL2H1cpoyAH","outputId":"67925eb5-811c-40af-c95b-0223ee5a5444","execution":{"iopub.status.busy":"2023-08-05T05:29:20.060161Z","iopub.execute_input":"2023-08-05T05:29:20.060832Z","iopub.status.idle":"2023-08-05T05:29:20.154950Z","shell.execute_reply.started":"2023-08-05T05:29:20.060793Z","shell.execute_reply":"2023-08-05T05:29:20.154147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.compile(loss=\"categorical_crossentropy\",\n                optimizer=\"adam\",\n                metrics=[\"accuracy\"])","metadata":{"id":"Q_ZQ2NygqeN4","execution":{"iopub.status.busy":"2023-08-05T05:29:20.156014Z","iopub.execute_input":"2023-08-05T05:29:20.156352Z","iopub.status.idle":"2023-08-05T05:29:20.176565Z","shell.execute_reply.started":"2023-08-05T05:29:20.156318Z","shell.execute_reply":"2023-08-05T05:29:20.175478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create char level datasets\ntrain_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\nval_char_dataset = tf.data.Dataset.from_tensor_slices((valid_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\ntest_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"id":"ttK7u-LdskKC","execution":{"iopub.status.busy":"2023-08-05T05:29:20.178153Z","iopub.execute_input":"2023-08-05T05:29:20.178761Z","iopub.status.idle":"2023-08-05T05:29:21.368498Z","shell.execute_reply.started":"2023-08-05T05:29:20.178725Z","shell.execute_reply":"2023-08-05T05:29:21.367458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_model_3 = model_3.fit(train_char_dataset,\n                              steps_per_epoch = int(0.1 * len(train_char_dataset)),\n                              epochs=3,\n                              validation_data=val_char_dataset,\n                              validation_steps=int(0.1 * len(val_char_dataset)))","metadata":{"id":"Kz13WaSit8gW","outputId":"ed6f0b51-3a2b-4167-cd61-a6ca4671c6d4","execution":{"iopub.status.busy":"2023-08-05T05:29:21.370098Z","iopub.execute_input":"2023-08-05T05:29:21.370486Z","iopub.status.idle":"2023-08-05T05:29:33.424656Z","shell.execute_reply.started":"2023-08-05T05:29:21.370450Z","shell.execute_reply":"2023-08-05T05:29:33.423625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make prediction character model\nmodel_3_pred_probs = model_3.predict(val_char_dataset)\nmodel_3_preds = tf.argmax(model_3_pred_probs, axis=1)","metadata":{"id":"84An6eW0ulIg","outputId":"036ee36f-e554-460b-98ec-6ee085ebc53c","execution":{"iopub.status.busy":"2023-08-05T05:29:33.426279Z","iopub.execute_input":"2023-08-05T05:29:33.426676Z","iopub.status.idle":"2023-08-05T05:29:36.386229Z","shell.execute_reply.started":"2023-08-05T05:29:33.426640Z","shell.execute_reply":"2023-08-05T05:29:36.385150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate results foe connv1d model\nmodel_3_results = calculate_results(y_true=val_labels_encoded,\n                                    y_pred= model_3_preds)","metadata":{"id":"A_XjcDoUvKZR","execution":{"iopub.status.busy":"2023-08-05T05:29:36.387901Z","iopub.execute_input":"2023-08-05T05:29:36.388278Z","iopub.status.idle":"2023-08-05T05:29:36.408022Z","shell.execute_reply.started":"2023-08-05T05:29:36.388242Z","shell.execute_reply":"2023-08-05T05:29:36.407037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3_results","metadata":{"id":"4h620GxzvfbV","outputId":"dcc819fe-86b0-4ff4-c236-a5eb7fde1856","execution":{"iopub.status.busy":"2023-08-05T05:29:36.409435Z","iopub.execute_input":"2023-08-05T05:29:36.410044Z","iopub.status.idle":"2023-08-05T05:29:36.416655Z","shell.execute_reply.started":"2023-08-05T05:29:36.410009Z","shell.execute_reply":"2023-08-05T05:29:36.415577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 4: Combining pretrained token embeddings + character embeddings(hybrid embedding layer)\n\n1. Create a token-level embedding(similar `model_1`)\n2. Create a character-level model(similar to `model_3` with slight modification)\n3. Combine 1 & 2 with concatenate(`layers.Concatenate`)\n4. Build a series of output layers on top of 3 similar to Figure 1 and section 4.2 of https://arxiv.org/pdf/1612.05251.pdf\n5. Construct a model which takes token and character-level sequences as input and produces sequence label probabilities as output","metadata":{"id":"0QR_fpvFvhOL"}},{"cell_type":"code","source":"# 1. Setup token inputs/model\ntoken_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\ntoken_embeddings = sentence_encoder_layer(token_inputs)\ntoken_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\ntoken_model = tf.keras.Model(inputs=token_inputs,\n                             outputs=token_outputs)\n\n# 2. Setup char inputs/model\nchar_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\nchar_vectors = char_vectorizer(char_inputs)\nchar_embeddings = char_embedding(char_vectors)\nchar_bi_lstm= layers.Bidirectional(layers.LSTM(24))(char_embeddings)\nchar_model = tf.keras.Model(inputs=char_inputs,\n                            outputs=char_bi_lstm)\n\n# 3. Concatenate token and char inputs(create hybrid token embedding)\ntoken_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output,\n                                                                  char_model.output])\n\n# 4. Create output layers - adding in Dropout in section of paper\ncombined_dropout = layers.Dropout(0.5)(token_char_concat)\ncombined_dense = layers.Dense(128, activation=\"relu\")(combined_dropout)\nfinal_dropout = layers.Dropout(0.5)(combined_dense)\noutput_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n\n# 5. Construct model with char and token inputs\nmodel_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n                         outputs=output_layer)\n","metadata":{"id":"T7nW50pAyK5p","execution":{"iopub.status.busy":"2023-08-05T05:29:36.418050Z","iopub.execute_input":"2023-08-05T05:29:36.418952Z","iopub.status.idle":"2023-08-05T05:29:39.871721Z","shell.execute_reply.started":"2023-08-05T05:29:36.418915Z","shell.execute_reply":"2023-08-05T05:29:39.870678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get summary\nmodel_4.summary()","metadata":{"id":"LThYBeB_4g-F","outputId":"9c040110-0562-40c3-899f-d9c2de85aab9","execution":{"iopub.status.busy":"2023-08-05T05:29:39.873030Z","iopub.execute_input":"2023-08-05T05:29:39.873388Z","iopub.status.idle":"2023-08-05T05:29:39.992990Z","shell.execute_reply.started":"2023-08-05T05:29:39.873353Z","shell.execute_reply":"2023-08-05T05:29:39.992020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot hybrid token and character model\nfrom keras.utils import plot_model\nplot_model(model_4, show_shapes=True)","metadata":{"id":"B-jM8hWa49J8","outputId":"dc0164bc-a360-44ec-f60a-a3fe30bcc657","execution":{"iopub.status.busy":"2023-08-05T05:29:39.994498Z","iopub.execute_input":"2023-08-05T05:29:39.995090Z","iopub.status.idle":"2023-08-05T05:29:40.321238Z","shell.execute_reply.started":"2023-08-05T05:29:39.995053Z","shell.execute_reply":"2023-08-05T05:29:40.320274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile token char model\nmodel_4.compile(loss=\"categorical_crossentropy\",\n                optimizer=\"adam\",\n                metrics=[\"accuracy\"])","metadata":{"id":"jDpSQCqj5wd0","execution":{"iopub.status.busy":"2023-08-05T05:29:40.322633Z","iopub.execute_input":"2023-08-05T05:29:40.323002Z","iopub.status.idle":"2023-08-05T05:29:40.338836Z","shell.execute_reply.started":"2023-08-05T05:29:40.322967Z","shell.execute_reply":"2023-08-05T05:29:40.337501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Combining token and character data into a tf.data Dataset","metadata":{"id":"trIKs9KF2oYr"}},{"cell_type":"code","source":"# Combine chars and tokens into a dataset\ntrain_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences,train_chars))\ntrain_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\ntrain_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels))\n\n# Prefetch and batch train data\ntrain_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"id":"NX783YkC3J4l","execution":{"iopub.status.busy":"2023-08-05T05:29:40.340922Z","iopub.execute_input":"2023-08-05T05:29:40.341848Z","iopub.status.idle":"2023-08-05T05:29:41.951882Z","shell.execute_reply.started":"2023-08-05T05:29:40.341814Z","shell.execute_reply":"2023-08-05T05:29:41.950863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine chars and tokens into a dataset\nval_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences,valid_chars))\nval_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\nval_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\n\n# Prefetch and batch val data\nval_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"id":"2Q0TXimH40Go","execution":{"iopub.status.busy":"2023-08-05T05:29:41.953200Z","iopub.execute_input":"2023-08-05T05:29:41.953551Z","iopub.status.idle":"2023-08-05T05:29:42.256976Z","shell.execute_reply.started":"2023-08-05T05:29:41.953518Z","shell.execute_reply":"2023-08-05T05:29:42.255994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check out training char and token embedding dataset\ntrain_char_token_dataset, val_char_token_dataset","metadata":{"id":"Fxy5F_0K5HUu","outputId":"02264109-ac2b-4090-b0de-4d34d844ee18","execution":{"iopub.status.busy":"2023-08-05T05:29:42.258225Z","iopub.execute_input":"2023-08-05T05:29:42.258579Z","iopub.status.idle":"2023-08-05T05:29:42.265199Z","shell.execute_reply.started":"2023-08-05T05:29:42.258544Z","shell.execute_reply":"2023-08-05T05:29:42.264036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting a model on token and character-level sequences","metadata":{"id":"zE-P8nlE5kRy"}},{"cell_type":"code","source":"# Fit the model on tokens and chars\nhistory_model_4 = model_4.fit(train_char_token_dataset,\n                              steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n                              epochs=3,\n                              validation_data=val_char_token_dataset,\n                              validation_steps=int(0.1 * len(val_char_token_dataset)))","metadata":{"id":"uQPB3MJv6J3e","outputId":"bfa1b48e-3a59-45eb-f17c-922637e0d284","execution":{"iopub.status.busy":"2023-08-05T05:29:42.266809Z","iopub.execute_input":"2023-08-05T05:29:42.267758Z","iopub.status.idle":"2023-08-05T05:33:20.154521Z","shell.execute_reply.started":"2023-08-05T05:29:42.267722Z","shell.execute_reply":"2023-08-05T05:33:20.153065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model\nmodel_4.evaluate(val_char_token_dataset)","metadata":{"id":"UYDrpAk762tr","execution":{"iopub.status.busy":"2023-08-05T05:33:20.156050Z","iopub.execute_input":"2023-08-05T05:33:20.156378Z","iopub.status.idle":"2023-08-05T05:34:16.662225Z","shell.execute_reply.started":"2023-08-05T05:33:20.156350Z","shell.execute_reply":"2023-08-05T05:34:16.661108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the predictions\nmodel_4_preds_probs = model_4.predict(val_char_token_dataset)","metadata":{"id":"f4mSgFBo6Amu","execution":{"iopub.status.busy":"2023-08-05T05:34:16.663978Z","iopub.execute_input":"2023-08-05T05:34:16.664347Z","iopub.status.idle":"2023-08-05T05:35:15.935630Z","shell.execute_reply.started":"2023-08-05T05:34:16.664307Z","shell.execute_reply":"2023-08-05T05:35:15.934446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Round the preds\npreds_model_4 = tf.argmax(model_4_preds_probs, axis=1)\npreds_model_4","metadata":{"id":"auXkgacI8rc5","execution":{"iopub.status.busy":"2023-08-05T05:35:15.937023Z","iopub.execute_input":"2023-08-05T05:35:15.937411Z","iopub.status.idle":"2023-08-05T05:35:15.950836Z","shell.execute_reply.started":"2023-08-05T05:35:15.937375Z","shell.execute_reply":"2023-08-05T05:35:15.949752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calulate the evaluation matrix\nmodel_4_results = calculate_results(y_pred=preds_model_4,\n                                    y_true=val_labels_encoded)\nmodel_4_results","metadata":{"id":"cy6L1D_S9BpL","execution":{"iopub.status.busy":"2023-08-05T05:35:15.952826Z","iopub.execute_input":"2023-08-05T05:35:15.953244Z","iopub.status.idle":"2023-08-05T05:35:15.984116Z","shell.execute_reply.started":"2023-08-05T05:35:15.953185Z","shell.execute_reply":"2023-08-05T05:35:15.983122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note**: Any engineered features used to train a model need to be available at test time. In our case, line numbers and total lines are available.","metadata":{"id":"IwGq6qcF_q9x"}},{"cell_type":"markdown","source":"### Create positional embeddings","metadata":{"id":"WjPmi0K0B2gh"}},{"cell_type":"code","source":"# How many different line numbers are there?\ntrain_df[\"line_number\"].value_counts()","metadata":{"id":"kHCZeLFuCBfQ","execution":{"iopub.status.busy":"2023-08-05T05:35:15.985470Z","iopub.execute_input":"2023-08-05T05:35:15.986098Z","iopub.status.idle":"2023-08-05T05:35:15.998142Z","shell.execute_reply.started":"2023-08-05T05:35:15.986057Z","shell.execute_reply":"2023-08-05T05:35:15.997108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the distribution of \"line_number\" column\ntrain_df.line_number.plot.hist()","metadata":{"id":"EsoI4WGOCNQ_","execution":{"iopub.status.busy":"2023-08-05T05:35:15.999630Z","iopub.execute_input":"2023-08-05T05:35:16.000222Z","iopub.status.idle":"2023-08-05T05:35:16.288625Z","shell.execute_reply.started":"2023-08-05T05:35:16.000187Z","shell.execute_reply":"2023-08-05T05:35:16.287632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use TensorFlow to create one-hot-encoded tensors of our \"line_number\" column\ntrain_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(),depth=15)\nval_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(),depth=15)\ntest_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(),depth=15)\ntrain_line_numbers_one_hot[:10], train_line_numbers_one_hot.shape","metadata":{"id":"J7YNr7R-C2ps","execution":{"iopub.status.busy":"2023-08-05T05:35:16.300628Z","iopub.execute_input":"2023-08-05T05:35:16.301445Z","iopub.status.idle":"2023-08-05T05:35:16.323964Z","shell.execute_reply.started":"2023-08-05T05:35:16.301415Z","shell.execute_reply":"2023-08-05T05:35:16.322956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"id":"b00T0nSLFc_E","execution":{"iopub.status.busy":"2023-08-05T05:35:16.326435Z","iopub.execute_input":"2023-08-05T05:35:16.327082Z","iopub.status.idle":"2023-08-05T05:35:16.339633Z","shell.execute_reply.started":"2023-08-05T05:35:16.327046Z","shell.execute_reply":"2023-08-05T05:35:16.338328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many total lines are there?\ntrain_df[\"total_lines\"].value_counts().sort_index()","metadata":{"id":"92T1Vu3OEIq5","execution":{"iopub.status.busy":"2023-08-05T05:35:16.342764Z","iopub.execute_input":"2023-08-05T05:35:16.343428Z","iopub.status.idle":"2023-08-05T05:35:16.358349Z","shell.execute_reply.started":"2023-08-05T05:35:16.343390Z","shell.execute_reply":"2023-08-05T05:35:16.357412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the distribution of \"line_number\" column\ntrain_df.total_lines.plot.hist()","metadata":{"id":"mRcfH5fGFjWk","execution":{"iopub.status.busy":"2023-08-05T05:35:16.359770Z","iopub.execute_input":"2023-08-05T05:35:16.360350Z","iopub.status.idle":"2023-08-05T05:35:16.660205Z","shell.execute_reply.started":"2023-08-05T05:35:16.360162Z","shell.execute_reply":"2023-08-05T05:35:16.659230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check teh coverage of a total line value of 20\nnp.percentile(train_df.total_lines, 98)","metadata":{"id":"on0ewpgmHaOj","execution":{"iopub.status.busy":"2023-08-05T05:35:16.661775Z","iopub.execute_input":"2023-08-05T05:35:16.662453Z","iopub.status.idle":"2023-08-05T05:35:16.671591Z","shell.execute_reply.started":"2023-08-05T05:35:16.662412Z","shell.execute_reply":"2023-08-05T05:35:16.670423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use TensorFlow to create one-hot-encoded tensors of our \"total_lines\" column\ntrain_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(),depth=20)\nval_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(),depth=20)\ntest_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(),depth=20)\ntrain_total_lines_one_hot[:10], train_total_lines_one_hot.shape","metadata":{"id":"sVkdQQH_GMuA","execution":{"iopub.status.busy":"2023-08-05T05:35:16.673531Z","iopub.execute_input":"2023-08-05T05:35:16.673955Z","iopub.status.idle":"2023-08-05T05:35:16.690485Z","shell.execute_reply.started":"2023-08-05T05:35:16.673919Z","shell.execute_reply":"2023-08-05T05:35:16.689233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building a tribrid embedding model\n\n1. Create a token-level model\n2. Create a character-level model\n3. Create a model for the \"line_number\" feature\n4. Create a model for the \"total_lines\"\nfeature\n5. Combine the outputs of 1 & 2 using tf.keras.layers.Concatenate\n6. Combine the outputs of 3, 4, 5 using tf.keras.layers.Concatenate\n7. Create an output layer to accept the tribrid embedding and output label probabilities\n8. Combine the inputs of 1,2,3,4 and outputs of into a tf.keras.Model","metadata":{"id":"_WSdeH4lGhRF"}},{"cell_type":"code","source":"# 1. Token inputs\ntoken_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\ntoken_embeddings = sentence_encoder_layer(token_inputs)\ntoken_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\ntoken_model = tf.keras.Model(inputs=token_inputs,\n                             outputs=token_outputs)\n\n# 2. Char inputs\nchar_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\nchar_vectors = char_vectorizer(char_inputs)\nchar_embeddings = char_embedding(char_vectors)\nchar_bi_lstm = layers.Bidirectional(layers.LSTM(24))(char_embeddings)\nchar_model = tf.keras.Model(inputs=char_inputs,\n                            outputs=char_bi_lstm)\n\n# 3. line numbers model\nline_inputs = layers.Input(shape=(15,) , dtype=tf.float32, name=\"line_numbers_inputs\")\nx = layers.Dense(32, activation=\"relu\")(line_inputs)\nline_number_model = tf.keras.Model(line_inputs, x)\n\n# 4. Total lines model\ntotal_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=\"total_line_inputs\")\ny = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\ntotal_line_model = tf.keras.Model(total_lines_inputs, y)\n\n# 5. Concatenate token and char inputs(create hybrid token embedding)\ncombined_embeddings = layers.Concatenate(name=\"char_token_hybrid_embedding\")([token_model.output,\n                                                                              char_model.output])\n\nz = layers.Dense(256, activation=\"relu\")(combined_embeddings)\nz = layers.Dropout(0.5)(z)\n\n# 6. Combine the positional embedding with combined token and char embedings\ntribrid_embeddings = layers.Concatenate(name=\"char_token_positional_embedding\")([line_number_model.output,\n                                                                                 total_line_model.output,\n                                                                                 z])\n\n# 7. Create output layer\noutput_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(tribrid_embeddings)\n\n# 8. Put together model with all kinds of inputs\nmodel_5 = tf.keras.Model(inputs=[line_number_model.input,\n                                 total_line_model.input,\n                                 token_model.input,\n                                 char_model.input],\n                         outputs=output_layer,\n                         name=\"model_5_tribrid_embedding_model\")","metadata":{"id":"_lC3luC_JGXg","execution":{"iopub.status.busy":"2023-08-05T05:35:16.693027Z","iopub.execute_input":"2023-08-05T05:35:16.693305Z","iopub.status.idle":"2023-08-05T05:35:20.706691Z","shell.execute_reply.started":"2023-08-05T05:35:16.693280Z","shell.execute_reply":"2023-08-05T05:35:20.705671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_5.summary()","metadata":{"id":"oHtC-TJDR4GB","execution":{"iopub.status.busy":"2023-08-05T05:35:20.707869Z","iopub.execute_input":"2023-08-05T05:35:20.708230Z","iopub.status.idle":"2023-08-05T05:35:20.845389Z","shell.execute_reply.started":"2023-08-05T05:35:20.708193Z","shell.execute_reply":"2023-08-05T05:35:20.844362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot model_5\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_5, show_shapes=True )","metadata":{"id":"ysU5hLsdQy8Y","execution":{"iopub.status.busy":"2023-08-05T05:35:20.847000Z","iopub.execute_input":"2023-08-05T05:35:20.847611Z","iopub.status.idle":"2023-08-05T05:35:21.025117Z","shell.execute_reply.started":"2023-08-05T05:35:20.847574Z","shell.execute_reply":"2023-08-05T05:35:21.024117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What is label smoothing?\n\nFor example, if our model gets too confident on a single class (e.g. its prediction probability is really high), it may get stuck on that class and not consider other classes...\n\nReally confident: `[0.0, 0.0, 1.0, 0.0, 0.0]`\n\nWhat label smoothing does is it assigns some of the value from the highest pred prob to other classes, in turn, hopefully improving generalization: `[0.02, 0.01, 0.96, 0.01, 0.01]`","metadata":{"id":"Ww5Y0wEM94s4"}},{"cell_type":"code","source":"# Compile token, char, and positional embedding model\nmodel_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), #helps to prevent overfitting\n                optimizer= \"adam\",\n                metrics=[\"accuracy\"])","metadata":{"id":"5E_AqdHlRocl","execution":{"iopub.status.busy":"2023-08-05T05:35:21.026507Z","iopub.execute_input":"2023-08-05T05:35:21.026957Z","iopub.status.idle":"2023-08-05T05:35:21.045416Z","shell.execute_reply.started":"2023-08-05T05:35:21.026922Z","shell.execute_reply":"2023-08-05T05:35:21.044503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create tribrid embedding datasets using tf.data","metadata":{"id":"VhdTiHmRDwr6"}},{"cell_type":"code","source":"train_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot, train_total_lines_one_hot, train_sentences, train_chars))\ntrain_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\ntrain_dataset = tf.data.Dataset.zip((train_data, train_labels))\n\nval_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,val_total_lines_one_hot,val_sentences,valid_chars))\nval_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\nval_dataset = tf.data.Dataset.zip((val_data, val_labels))\n\ntest_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot, test_total_lines_one_hot, test_sentences, test_chars))\ntest_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\ntest_dataset = tf.data.Dataset.zip((test_data, test_labels))\n\n# Prefetch and batch data\ntrain_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\nval_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"id":"7OT78J0UD5rg","execution":{"iopub.status.busy":"2023-08-05T05:35:21.046990Z","iopub.execute_input":"2023-08-05T05:35:21.047382Z","iopub.status.idle":"2023-08-05T05:35:23.273175Z","shell.execute_reply.started":"2023-08-05T05:35:21.047348Z","shell.execute_reply":"2023-08-05T05:35:23.272121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset, val_dataset","metadata":{"id":"UDodP6TnHS4s","execution":{"iopub.status.busy":"2023-08-05T05:35:23.274766Z","iopub.execute_input":"2023-08-05T05:35:23.275120Z","iopub.status.idle":"2023-08-05T05:35:23.285835Z","shell.execute_reply.started":"2023-08-05T05:35:23.275084Z","shell.execute_reply":"2023-08-05T05:35:23.284835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting, evaluating and making prediction with tribrid model","metadata":{"id":"qL_cE2LiHqcD"}},{"cell_type":"code","source":"history_model_5 =  model_5.fit(train_dataset,\n                               epochs=3,\n                               steps_per_epoch= int(0.1* len(train_dataset)),\n                               validation_data=(val_dataset),\n                               validation_steps=int(0.1 * len(val_dataset)))","metadata":{"id":"URrSZycGI06U","execution":{"iopub.status.busy":"2023-08-05T05:35:23.287481Z","iopub.execute_input":"2023-08-05T05:35:23.288161Z","iopub.status.idle":"2023-08-05T05:38:35.047461Z","shell.execute_reply.started":"2023-08-05T05:35:23.288122Z","shell.execute_reply":"2023-08-05T05:38:35.046302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make prediction with the char token pos model\nmodel_5_pred_probs = model_5.predict(val_dataset, verbose=1)\n# Conver pred probs to pred labels\nmodel_5_preds = tf.argmax(model_5_pred_probs, axis=1)\nmodel_5_preds","metadata":{"id":"4wsTHt-VJREc","execution":{"iopub.status.busy":"2023-08-05T05:38:35.049157Z","iopub.execute_input":"2023-08-05T05:38:35.049668Z","iopub.status.idle":"2023-08-05T05:39:35.986117Z","shell.execute_reply.started":"2023-08-05T05:38:35.049629Z","shell.execute_reply":"2023-08-05T05:39:35.984919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the results of char token pos model\nmodel_5_results = calculate_results(y_true=val_labels_encoded,\n                                    y_pred=model_5_preds)\nmodel_5_results","metadata":{"id":"VRc9foMRK_FO","execution":{"iopub.status.busy":"2023-08-05T05:39:35.987966Z","iopub.execute_input":"2023-08-05T05:39:35.988357Z","iopub.status.idle":"2023-08-05T05:39:36.012989Z","shell.execute_reply.started":"2023-08-05T05:39:35.988318Z","shell.execute_reply":"2023-08-05T05:39:36.011823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compare model results","metadata":{"id":"Skz5a_Z3cUHo"}},{"cell_type":"code","source":"# Combine model results into a dataframe\nall_model_results = pd.DataFrame({\"model_0_baseline\": baseline_results,\n                                 \"model_1_custom_token_embedding\": model_1_results,\n                                 \"model_2_pretrained_token_embedding\": model_2_results,\n                                 \"model_3_custom_char_embedding\": model_3_results,\n                                 \"model_4_hybrid_char_token_embedding\": model_4_results,\n                                 \"model_5_pos_char_token_embedding\": model_5_results})\n\nall_model_results = all_model_results.transpose()\nall_model_results","metadata":{"execution":{"iopub.status.busy":"2023-08-05T05:52:19.331696Z","iopub.execute_input":"2023-08-05T05:52:19.332085Z","iopub.status.idle":"2023-08-05T05:52:19.349684Z","shell.execute_reply.started":"2023-08-05T05:52:19.332052Z","shell.execute_reply":"2023-08-05T05:52:19.348425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reduce the accuracy to same scale as other metrics\nall_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100","metadata":{"execution":{"iopub.status.busy":"2023-08-05T05:54:45.801305Z","iopub.execute_input":"2023-08-05T05:54:45.801747Z","iopub.status.idle":"2023-08-05T05:54:45.808168Z","shell.execute_reply.started":"2023-08-05T05:54:45.801713Z","shell.execute_reply":"2023-08-05T05:54:45.807042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot and compare all model results\nall_model_results.plot(kind=\"bar\", figsize=(10,7)).legend(bbox_to_anchor=(1.0,1.0));","metadata":{"execution":{"iopub.status.busy":"2023-08-05T05:56:57.248073Z","iopub.execute_input":"2023-08-05T05:56:57.248485Z","iopub.status.idle":"2023-08-05T05:56:57.688034Z","shell.execute_reply.started":"2023-08-05T05:56:57.248451Z","shell.execute_reply":"2023-08-05T05:56:57.687105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort model results by f1_score\nall_model_results.sort_values(\"f1\", ascending=True)[\"f1\"].plot(kind=\"bar\", figsize=(10,7))","metadata":{"execution":{"iopub.status.busy":"2023-08-05T06:00:45.007090Z","iopub.execute_input":"2023-08-05T06:00:45.008141Z","iopub.status.idle":"2023-08-05T06:00:45.398683Z","shell.execute_reply.started":"2023-08-05T06:00:45.008102Z","shell.execute_reply":"2023-08-05T06:00:45.397699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save and load model","metadata":{}},{"cell_type":"code","source":"# Save the best performing model to SaveModel format (default)\nmodel_5.save(\"skimlit_tribrid_model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-05T06:06:09.548053Z","iopub.execute_input":"2023-08-05T06:06:09.548457Z","iopub.status.idle":"2023-08-05T06:08:06.235361Z","shell.execute_reply.started":"2023-08-05T06:06:09.548425Z","shell.execute_reply":"2023-08-05T06:08:06.234188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load in the best performing model\nloaded_model = tf.keras.models.load_model(\"skimlit_tribrid_model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-05T06:08:06.478371Z","iopub.execute_input":"2023-08-05T06:08:06.478738Z","iopub.status.idle":"2023-08-05T06:09:04.225949Z","shell.execute_reply.started":"2023-08-05T06:08:06.478701Z","shell.execute_reply":"2023-08-05T06:09:04.224725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions with the loaded model on the validation set\nloaded_model_preds_probs = loaded_model.predict(val_dataset)\nloaded_preds = tf.argmax(loaded_model_preds_probs, axis=1)\nloaded_preds[:10]","metadata":{"execution":{"iopub.status.busy":"2023-08-05T06:19:10.192865Z","iopub.execute_input":"2023-08-05T06:19:10.193360Z","iopub.status.idle":"2023-08-05T06:20:18.457632Z","shell.execute_reply.started":"2023-08-05T06:19:10.193319Z","shell.execute_reply":"2023-08-05T06:20:18.456214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the loaded model results\nloaded_model_results = calculate_results(y_true=val_labels_encoded,\n                                        y_pred=loaded_preds)\nloaded_model_results","metadata":{"execution":{"iopub.status.busy":"2023-08-05T06:21:15.793688Z","iopub.execute_input":"2023-08-05T06:21:15.794260Z","iopub.status.idle":"2023-08-05T06:21:15.830888Z","shell.execute_reply.started":"2023-08-05T06:21:15.794204Z","shell.execute_reply":"2023-08-05T06:21:15.829628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_5_results ","metadata":{"execution":{"iopub.status.busy":"2023-08-05T06:23:03.960841Z","iopub.execute_input":"2023-08-05T06:23:03.961333Z","iopub.status.idle":"2023-08-05T06:23:03.975911Z","shell.execute_reply.started":"2023-08-05T06:23:03.961291Z","shell.execute_reply":"2023-08-05T06:23:03.974729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T06:24:11.271331Z","iopub.execute_input":"2023-08-05T06:24:11.272165Z","iopub.status.idle":"2023-08-05T06:24:11.450283Z","shell.execute_reply.started":"2023-08-05T06:24:11.272117Z","shell.execute_reply":"2023-08-05T06:24:11.449259Z"},"trusted":true},"execution_count":null,"outputs":[]}]}